搭建模块化神经网络八股：
（1）前向传播就是搭建网络，设计网络结构（forward.py）
完成网络结构设计
def forward(x,regularizer):
    w=
    b=
    y=
    return y

def get_weight(shape,regularizer):#w相关
    w=tf.Variable()
    tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))

def get_bias(shape):#b相关
    b=tf.Variable()
    return b

（2）反向传播就是训练网络，优化网络参数（backward.py）
def backward():
    x=tf.placeholder()
    y_tf.placeholder()
    y=forward.forward(x,REGULARIZER)
    global_step=tf.Variable(0,trainable=False)

    #正则化：
    loss可以是
    均方误差：tf.reduce_mean(tf.square(y-y_))
    交叉熵ce:tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))
           y与y_的差距：tf.reduce_mean(ce)

    加入正则化后：
    loss=y与y_的差距+tf.add_n(tf.get_collection('losses'))

    #指数衰减学习率
    learning_train=tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,数据集总样本数/BATCH_SIZE,LEARNING_RATE_DECAY,staircase=True)


    #滑动平均
    tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
    MOVING_AVERAGE_DECAY=0.99
    ema=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
    ema_op=ema.apply(tf.trainable_variables())
    with tf.control_dependencies([train_step,ema_op]):
        train_op=tf.no_op(name='train')

    with tf.Session() as sess:
        init_op=tf.global_variables_initializer()
        sess.run(init_op)
        for i in range(STEPS):
            sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})
            if i%轮数==0:
                loss_v=sess.run(loss_mse,feed_dict={x:X,y_:Y_})
                print('After %d steps, loss is :%f'%(i,loss_v))

if __name__ == '__main__':
    backward()